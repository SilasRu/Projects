{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Q-Learning Taxi example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space = 6 possible Actions  \n",
    "1 south  \n",
    "2 north  \n",
    "3 east  \n",
    "4 west  \n",
    "5 pickup  \n",
    "6 dropoff  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2').env\n",
    "\n",
    "#Reset the Environment to random state\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State space = 500 States  \n",
    "4 pickup locations  \n",
    "5x5 grid  \n",
    "1 additional passenger (x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding a State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (taxi row, taxi column, passenger index, destination index)\n",
    "state = env.encode(3, 1, 2, 0)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{action: [(probability, nextstate, reward, done)]}  \n",
    "0-5 corresponds to the actions (south, north, east, west, pickup, dropoff)  \n",
    "Probability is always 1.0  \n",
    "nextstate is state of action at this index of the dict  \n",
    "movements have a -1 reward  \n",
    "pickup/dropoff have -10, at right location +20 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 328, -1, False)],\n",
       " 1: [(1.0, 128, -1, False)],\n",
       " 2: [(1.0, 248, -1, False)],\n",
       " 3: [(1.0, 208, -1, False)],\n",
       " 4: [(1.0, 228, -10, False)],\n",
       " 5: [(1.0, 228, -10, False)]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[228]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.  \n",
    "In our Taxi environment, we have the reward table, P, that the agent will learn from. It takes an action in the current state, recieves the reward for this action and then updating a Q-value to remember if that action was beneficial.\n",
    "\n",
    "The values store in the Q-table are called a Q-values, and they map to a (state, action) combination.\n",
    "\n",
    "Better Q-values imply better chances of getting greater rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Q-table to a 500x6 Matrix of 0\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Q(state, action) = \\small{(1-\\alpha) Q (state,action) + \\alpha(reward + \\gamma maxQ(nextState, allActions)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ is the learning rate (between 0 and 1), the extent to which the Q-values are being updated in every iteration.\n",
    "\n",
    "$\\gamma$ is the discount factor (between 0 and 1), determines how important the future rewards are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assigning (←), or updating, the Q-value of the agent's current state and action by first taking a weight (1−α) of the old Q-value, then adding the learned value. \n",
    "\n",
    "The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting learned values\n",
    "\n",
    "After enough random exploration of actions, the Q-values converge, serving as actoun-value function, where the agent can exploit the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, with the parameter ϵ \n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n"
     ]
    }
   ],
   "source": [
    "# Training the agent\n",
    "\n",
    "# Plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "frames = [] # for animation\n",
    "total_reward_list = []\n",
    "avg_reward = [0]\n",
    "total_reward, avg_reward_tot, avg_reward_episodes, total_epochs, episodes = 0,0,0,0,0\n",
    "\n",
    "# Training loop\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Setting initial values\n",
    "    epochs, penalties, reward = 0,0,0\n",
    "    done = False\n",
    "\n",
    "    # Decide wether to pick a random action or exploit the already computed Q-values (comparing to epsilon)\n",
    "    while not done:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            # Explore the action space (Number between 1 and 6)\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit learned values (Maximum in Q_table of current state)\n",
    "            action = np.argmax(q_table[state]) \n",
    "        \n",
    "        #Execute the chosen action to obtain next state and reward\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update the Q Value to the new Q Value (of next state)\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        # Q-learning equation:\n",
    "        # (1-alpha) * Q(state, action) + alpha(reward_t+1 + gamma * maxQ(state_t+1, action))\n",
    "        new_value = (1-alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        \n",
    "        q_table[state, action]= new_value\n",
    "        \n",
    "        # Update Penalties and epochs\n",
    "        if reward == -10:\n",
    "            penalties +=1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'avg_reward_tot': avg_reward_tot,\n",
    "            'avg_reward': avg_reward_episodes\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Update state and reward\n",
    "        state = next_state\n",
    "        epochs +=1\n",
    "        total_epochs+=1\n",
    "        total_reward+=reward\n",
    "        total_reward_list.append(reward)\n",
    "        avg_reward_tot = total_reward/total_epochs\n",
    "        avg_reward_episodes = avg_reward[episodes]\n",
    "\n",
    "    # Clear output after 100 episodes\n",
    "    if i%100 ==0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "        episodes +=1\n",
    "        avg_reward.append(sum(total_reward_list)/len(total_reward_list))\n",
    "        total_reward_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print frames function for Q-Learning vizualisation      \n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        print(f\"Avg. reward (100 episodes): {frame['avg_reward']}\")\n",
    "        print(f\"Avg. total reward: {frame['avg_reward_tot']}\")\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents performance on the first few runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "Timestep: 100\n",
      "State: 133\n",
      "Action: 2\n",
      "Reward: -1\n",
      "Avg. reward (100 episodes): 0\n",
      "Avg. total reward: -2.5454545454545454\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames[0:100])\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents Performance after 50k episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Timestep: 100\n",
      "State: 101\n",
      "Action: 1\n",
      "Reward: -1\n",
      "Avg. reward (100 episodes): 0.22505307855626328\n",
      "Avg. total reward: 0.00892458487298763\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames[round(len(frames)/2):round(len(frames)/2)+100])\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents Performance at 100k episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 200\n",
      "State: 97\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Avg. reward (100 episodes): 0.24264178033022255\n",
      "Avg. total reward: 0.11515231823180994\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Console output (Last 1000 training runs)\n",
    "print_frames(frames[len(frames)-200:len(frames)])\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Values at the illustrations state\n",
    "1 south  \n",
    "2 north  \n",
    "3 east  \n",
    "4 west  \n",
    "5 pickup  \n",
    "6 dropoff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.30435943, -1.97092096, -2.30096055, -2.22305168, -9.79587531,\n",
       "       -9.6383713 ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "env.s = state\n",
    "env.render()\n",
    "env.P[328]\n",
    "\n",
    "q_table[328]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
