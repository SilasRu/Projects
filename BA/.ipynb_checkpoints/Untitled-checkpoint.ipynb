{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning for trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(col='close'):\n",
    "  # Returns a 3 x n_step array\n",
    "  msft = pd.read_csv('data/daily_MSFT.csv', usecols=[col])\n",
    "  ibm = pd.read_csv('data/daily_IBM.csv', usecols=[col])\n",
    "  qcom = pd.read_csv('data/daily_QCOM.csv', usecols=[col])\n",
    "  # recent price are at top; reverse it\n",
    "  return np.array([msft[col].values[::-1],\n",
    "                   ibm[col].values[::-1],\n",
    "                   qcom[col].values[::-1]])\n",
    "\n",
    "\n",
    "def get_scaler(env):\n",
    "  # Takes a env and returns a scaler for its observation space\n",
    "  low = [0] * (env.n_stock * 2 + 1)\n",
    "\n",
    "  high = []\n",
    "  max_price = env.stock_price_history.max(axis=1)\n",
    "  min_price = env.stock_price_history.min(axis=1)\n",
    "  max_cash = env.init_invest * 3 # 3 is a magic number...\n",
    "  max_stock_owned = max_cash // min_price\n",
    "  for i in max_stock_owned:\n",
    "    high.append(i)\n",
    "  for i in max_price:\n",
    "    high.append(i)\n",
    "  high.append(max_cash)\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  scaler.fit([low, high])\n",
    "  return scaler\n",
    "\n",
    "\n",
    "def maybe_make_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MultiDiscrete(gym.Env):\n",
    "    def __init__(self, nvec):\n",
    "        \"\"\"\n",
    "        nvec: vector of counts of each categorical variable\n",
    "        \"\"\"\n",
    "        assert (np.array(nvec) > 0).all(), 'nvec (counts) have to be positive'\n",
    "        self.nvec = np.asarray(nvec, dtype=np.uint32)\n",
    "\n",
    "        super(MultiDiscrete, self).__init__(self.nvec.shape, np.uint32)\n",
    "        self.np_random = np.random.RandomState()\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self.np_random.seed(seed)\n",
    "\n",
    "    def sample(self):\n",
    "        return (self.np_random.random_sample(self.nvec.shape) * self.nvec).astype(self.dtype)\n",
    "\n",
    "    def contains(self, x):\n",
    "        # if nvec is uint32 and space dtype is uint32, then 0 <= x < self.nvec guarantees that x\n",
    "        # is within correct bounds for space dtype (even though x does not have to be unsigned)\n",
    "        return (0 <= x).all() and (x < self.nvec).all()\n",
    "\n",
    "    def to_jsonable(self, sample_n):\n",
    "        return [sample.tolist() for sample in sample_n]\n",
    "\n",
    "    def from_jsonable(self, sample_n):\n",
    "        return np.array(sample_n)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"MultiDiscrete({})\".format(self.nvec)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return np.all(self.nvec == other.nvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Trading environment\n",
    "  A 3-stock (MSFT, IBM, QCOM) trading environment.\n",
    "  \n",
    "  State: [Numbers of stocks owned, current stock prices, cash in hand]\n",
    "    - array of length n_stock * 2 + 1\n",
    "    - price is discretized (to integer) to reduce state space\n",
    "    - use close price for each stock\n",
    "    - cash in hand is evaluated at each step based on action performed\n",
    "  \n",
    "  Action: sell (0), hold (1), and buy (2)\n",
    "    - when selling, sell all the shares\n",
    "    - when buying, buy as many as cash in hand allows\n",
    "    - if buying multiple stock, equally distribute cash in hand and then\n",
    "      utilize the balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, train_data, init_invest=20000):\n",
    "        # data\n",
    "        self.stock_price_history = np.around(train_data) # round up to integer to reduce state space\n",
    "        self.n_stock, self.n_step = self.stock_price_history.shape\n",
    "\n",
    "        # instance attributes\n",
    "        self.init_invest = init_invest\n",
    "        self.cur_step = None\n",
    "        self.stock_owned = None\n",
    "        self.stock_price = None\n",
    "        self.cash_in_hand = None\n",
    "\n",
    "        # action space\n",
    "        self.action_space = spaces.Discrete(3**self.n_stock)\n",
    "\n",
    "        # observation space: give estimates in order to sample and build scaler\n",
    "        stock_max_price = self.stock_price_history.max(axis=1)\n",
    "        stock_range = [[0, init_invest * 2 // mx] for mx in stock_max_price]\n",
    "        price_range = [[0, mx] for mx in stock_max_price]\n",
    "        cash_in_hand_range = [[0, init_invest * 2]]\n",
    "        self.observation_space = spaces.MultiDiscrete(stock_range + price_range + cash_in_hand_range)\n",
    "\n",
    "        # seed and start\n",
    "        self._seed()\n",
    "        self._reset()\n",
    "\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "    def _reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.stock_owned = [0] * self.n_stock\n",
    "        self.stock_price = self.stock_price_history[:, self.cur_step]\n",
    "        self.cash_in_hand = self.init_invest\n",
    "        return self._get_obs()\n",
    "\n",
    "\n",
    "    def _step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "        prev_val = self._get_val()\n",
    "        self.cur_step += 1\n",
    "        self.stock_price = self.stock_price_history[:, self.cur_step] # update price\n",
    "        self._trade(action)\n",
    "        cur_val = self._get_val()\n",
    "        reward = cur_val - prev_val\n",
    "        done = self.cur_step == self.n_step - 1\n",
    "        info = {'cur_val': cur_val}\n",
    "        return self._get_obs(), reward, done, info\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = []\n",
    "        obs.extend(self.stock_owned)\n",
    "        obs.extend(list(self.stock_price))\n",
    "        obs.append(self.cash_in_hand)\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def _get_val(self):\n",
    "        return np.sum(self.stock_owned * self.stock_price) + self.cash_in_hand\n",
    "\n",
    "\n",
    "    def _trade(self, action):\n",
    "        # all combo to sell(0), hold(1), or buy(2) stocks\n",
    "        action_combo = map(list, itertools.product([0, 1, 2], repeat=self.n_stock))\n",
    "        action_vec = action_combo[action]\n",
    "\n",
    "        # one pass to get sell/buy index\n",
    "        sell_index = []\n",
    "        buy_index = []\n",
    "        for i, a in enumerate(action_vec):\n",
    "            if a == 0:\n",
    "                sell_index.append(i)\n",
    "            elif a == 2:\n",
    "                buy_index.append(i)\n",
    "\n",
    "        # two passes: sell first, then buy; might be naive in real-world settings\n",
    "        if sell_index:\n",
    "            for i in sell_index:\n",
    "                self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
    "                self.stock_owned[i] = 0\n",
    "        if buy_index:\n",
    "            can_buy = True\n",
    "            while can_buy:\n",
    "                for i in buy_index:\n",
    "                    if self.cash_in_hand > self.stock_price[i]:\n",
    "                        self.stock_owned[i] += 1 # buy one share\n",
    "                        self.cash_in_hand -= self.stock_price[i]\n",
    "                else:\n",
    "                    can_buy = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(n_obs, n_action, n_hidden_layer=1, n_neuron_per_layer=32,activation='relu', loss='mse'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neuron_per_layer, input_dim=n_obs, activation=activation))\n",
    "    \n",
    "    for _ in range(n_hidden_layer):\n",
    "        model.add(Dense(n_neuron_per_layer, activation=activation))\n",
    "        model.add(Dense(n_action, activation='linear'))\n",
    "        model.compile(loss=loss, optimizer=Adam())\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "  # A simple Deep Q agent\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = mlp(state_size, action_size)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "\n",
    "    def replay(self, batch_size=32):\n",
    "    # vectorized implementation; 30x speed up compared with for loop\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        states = np.array([tup[0][0] for tup in minibatch])\n",
    "        actions = np.array([tup[1] for tup in minibatch])\n",
    "        rewards = np.array([tup[2] for tup in minibatch])\n",
    "        next_states = np.array([tup[3][0] for tup in minibatch])\n",
    "        done = np.array([tup[4] for tup in minibatch])\n",
    "\n",
    "        # Q(s', a)\n",
    "        target = rewards + self.gamma * np.amax(self.model.predict(next_states), axis=1)\n",
    "        # end state target is reward itself (no lookahead)\n",
    "        target[done] = rewards[done]\n",
    "\n",
    "        # Q(s, a)\n",
    "        target_f = self.model.predict(states)\n",
    "        # make the agent to approximately map the current state to future discounted reward\n",
    "        target_f[range(batch_size), actions] = target\n",
    "\n",
    "        self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "nvec (counts) have to be positive",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-0bc48f9ca7fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3526\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTradingEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_initial_invest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mstate_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0maction_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-ffe8e6ec0e2f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_data, init_invest)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprice_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstock_max_price\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mcash_in_hand_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_invest\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDiscrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_range\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprice_range\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcash_in_hand_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# seed and start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\spaces\\multi_discrete.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, nvec)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnvec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \"\"\"\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnvec\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nvec (counts) have to be positive'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: nvec (counts) have to be positive"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-e', '--episode', type=int, default=2000,\n",
    "#                       help='number of episode to run')\n",
    "#     parser.add_argument('-b', '--batch_size', type=int, default=32,\n",
    "#                       help='batch size for experience replay')\n",
    "#     parser.add_argument('-i', '--initial_invest', type=int, default=20000,\n",
    "#                       help='initial investment amount')\n",
    "#     parser.add_argument('-m', '--mode', type=str, required=True,\n",
    "#                       help='either \"train\" or \"test\"')\n",
    "#     parser.add_argument('-w', '--weights', type=str, help='a trained model weights')\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "    args_initial_invest = 20000\n",
    "    args_episode = 2000\n",
    "    args_batch_size = 32\n",
    "    args_mode = 'train'\n",
    "    args_weights = None\n",
    "    \n",
    "    maybe_make_dir('weights')\n",
    "    maybe_make_dir('portfolio_val')\n",
    "\n",
    "    timestamp = time.strftime('%Y%m%d%H%M')\n",
    "\n",
    "    data = np.around(get_data())\n",
    "    train_data = data[:, :3526]\n",
    "    test_data = data[:, 3526:]\n",
    "\n",
    "    env = TradingEnv(train_data, args_initial_invest)\n",
    "    state_size = env.observation_space.shape\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scaler = get_scaler(env)\n",
    "\n",
    "    portfolio_value = []\n",
    "\n",
    "    if args_mode == 'test':\n",
    "        # remake the env with test data\n",
    "        env = TradingEnv(test_data, args_initial_invest)\n",
    "        # load trained weights\n",
    "        agent.load(args_weights)\n",
    "        # when test, the timestamp is same as time when weights was trained\n",
    "        timestamp = re.findall(r'\\d{12}', args_weights)[0]\n",
    "\n",
    "    for e in range(args_episode):\n",
    "        state = env.reset()\n",
    "        state = scaler.transform([state])\n",
    "        for time in range(env.n_step):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = scaler.transform([next_state])\n",
    "            if args_mode == 'train':\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, episode end value: {}\".format(\n",
    "                      e + 1, args_episode, info['cur_val']))\n",
    "                portfolio_value.append(info['cur_val']) # append episode end portfolio value\n",
    "                break\n",
    "            if args_mode == 'train' and len(agent.memory) > args_batch_size:\n",
    "                agent.replay(args_batch_size)\n",
    "        if args_mode == 'train' and (e + 1) % 10 == 0:  # checkpoint weights\n",
    "            agent.save('weights/{}-dqn.h5'.format(timestamp))\n",
    "\n",
    "        # save portfolio value history to disk\n",
    "        with open('portfolio_val/{}-{}.p'.format(timestamp, args_mode), 'wb') as fp:\n",
    "            pickle.dump(portfolio_value, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.around(get_data())\n",
    "# train_data = data[:, :3526]\n",
    "# test_data = data[:, 3526:]\n",
    "\n",
    "# TradingEnv(train_data, args_initial_invest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "nvec (counts) have to be positive",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-56680464dbf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTradingEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs_initial_invest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-81f9609d17bd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_data, init_invest)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprice_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstock_max_price\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mcash_in_hand_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_invest\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDiscrete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_range\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprice_range\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcash_in_hand_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# seed and start\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\spaces\\multi_discrete.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, nvec)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnvec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \"\"\"\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnvec\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nvec (counts) have to be positive'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnvec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: nvec (counts) have to be positive"
     ]
    }
   ],
   "source": [
    "env = TradingEnv(train_data, args_initial_invest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
